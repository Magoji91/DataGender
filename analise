library(readxl)
data18 <- read_excel("C:/Users/smsanda/Pictures/data18.xlsx")
View(data18)

data<-data18

library(dplyr)
library(moments)

head(data) # Print the first few, or last few, lines of a mdm object
str(data) #check the variables / Get a summary of an object’s structure
summary(data) #Get more detailed information out a model.
dim(data) #check dimesions ( number of row & columns) in data set
ls(data) # List all variables in the environment.
table(is.na(data)) # See counts of missing values
colSums(is.na(data)) # missing value in data exploration stages.
unique(data) #See unique values.
pairs(data)

#You can use the loglm( ) function in the MASS package to produce log-linear models. 
library(MASS)
attach(data)

A<-data$gender # Access the values
table(A) #table uses the cross-classifying factors to build a contingency table of the counts at each combination of factor levels.
table(A, useNA = "ifany") #if you want to count the NA's
summary(A)
sd(A) #grau de variação de um conjunto de dados
var(A) #indicador de variabilidade dos dados X Graus de liberdade é a diferença entre a dimensão da amostra (n) e a quantidade de parâmetros a serem avaliados na população
CVA=(sd(A)/mean(A))*100 #indica a quantidade de variação de um conjunto de dados em relação a média
print(CVA)
SKA = (3*(mean(A) - median(A))/(sd(A))) #Sk≈0: dados simétricos. Tanto a cauda do lado direito quanto a do lado esquerdo da função densidade de probabilidade são iguais // Sk<0: assimetria negativa. A cauda do lado esquerdo da função densidade de probabilidade é maior que a do lado direito. // Sk>0: assimetria positiva. A cauda do lado direito da função densidade de probabilidade é maior que a do lado esquerdo.
print(SKA)
kurtosis(A) #Ck≈0: Distribuição normal. Chamada de Curtose Mesocúrtica. // Ck<0: Cauda mais leve que a normal. Para um coeficiente de Curtose negativo, tem-se uma Curtose Platicúrtica. // Ck>0: Cauda mais pesada que a normal. Para um coeficiente de Curtose positivo, tem-se uma Curtose Leptocúrtica.


B<-data$birthyear
table(B)
table(B, useNA = "ifany")
summary(B)
sd(B)
var(B)
CVB=(sd(B)/mean(B))*100
print(CVB)
SKB = (3*(mean(B) - median(B))/(sd(B)))
print(SKB)
kurtosis(B)

C<-data$country
table(C)
table(C, useNA = "ifany")
summary(C)
sd(C)
var(C)
CVC=(sd(C)/mean(C))*100
print(CVC)
SKC = (3*(mean(C) - median(C))/(sd(C)))
print(SKC)
kurtosis(C)

D<-data$education
table(D)
table(D, useNA = "ifany")
summary(D)
sd(D)
var(D)
CVD=(sd(D)/mean(D))*100
print(CVD)
SKD = (3*(mean(D) - median(D))/(sd(D)))
print(SKD)
kurtosis(D)

#PLOTS

plot(data$gender)
barplot(data$gender)
plot(table(data$gender))
barplot(table(data$gender))

plot(data$education)
barplot(data$education)
plot(table(data$education))
barplot(table(data$education))
 
plot(data$country)
barplot(data$country)
plot(table(data$country))
barplot(table(data$country))
 
plot(data$birthyear)
barplot(data$birthyear)
plot(table(data$birthyear))
barplot(table(data$birthyear))


#Desempenho dos homens nas questões de MATEMÁTICA
library(dplyr)

genmasc<-data %>% filter(gender == 1, consent ==1)
M1<-table(genmasc$I ==1)
M2<-table(genmasc$II ==1)
M3<-table(genmasc$III ==1)
M4<-table(genmasc$IV ==1)
M5<-table(genmasc$V ==1)

mm<-rowSums(cbind(M1, M2, M3, M4, M5), na.rm=TRUE)
head(mm)

#Desempenho das mulheres nas questões de MATEMÁTICA
genfem<-data %>% filter(gender == 2, consent ==1)
F1<-table(genfem$I ==1)
F2<-table(genfem$II ==1)
F3<-table(genfem$III ==1)
F4<-table(genfem$IV ==1)
F5<-table(genfem$V ==1)

mf<-rowSums(cbind(F1, F2, F3, F4, F5), na.rm=TRUE)
head(mf)


      Wrong	 False   
Men	   186    199  
WomeN  193	   197 


AM = matrix( 
          c(186, 199, 193, 197), # the data elements 
            nrow=2,              # number of rows 
            ncol=2,              # number of columns 
             byrow = TRUE)        # fill matrix by rows 
chisq.test(AM) 

t.test(mm,mf,paired=TRUE)

var.test(mm, mf)

# A large p-value (p> 0.05) indicates weak evidence against the null hypothesis, so you fail to reject the null hypothesis.
#A negative t-value indicates a reversal in the directionality of the effect, which has no bearing on the significance of the difference between groups. Analysis of a negative t-value requires examination of its absolute value in comparison to the value on a table of t-values and degrees of freedom, which quantifies the variability of the final estimated number. If the absolute value of the experimental t-value is smaller than the value found on the degrees of freedom chart, then the means of the two groups can be said to be significantly different.
# The p-value of F-test is p = 0.3801 which is greater than the significance level 0.05. In conclusion, there is no significant difference between the two variances

#p <= alpha: reject H0, different distribution.
#p > alpha: fail to reject H0, same distribution.
#Type I Error. Reject the null hypothesis when there is in fact no significant effect (false positive). The p-value is optimistically small.
#Type II Error. Not reject the null hypothesis when there is a significant effect (false negative). The p-value is pessimistically large.

write.csv(matrix(as.numeric(mm),nrow=1),row.names=FALSE)
write.csv(matrix(as.numeric(mf),nrow=1),row.names=FALSE)

var.test(mm,mf)
#qf(p, df.num, df.den)
qf(0.95, 1, 1)

#We obtained p-value greater than 0.05, then we can assume that the two variances are homogeneous. Indeed we can compare the value of F obtained with the tabulated value of F for alpha = 0.05, degrees of freedom of numerator = 4, and degrees of freedom of denominator = 4, using the function qf(p, df.num, df.den): Note that the value of F computed is less than the tabulated value of F, which leads us to accept the null hypothesis of homogeneity of variances.


t.test(mm,mf, var.equal=TRUE, paired=FALSE)
qt(0.975, 2)
qt(0.95, 2)

#We obtained p-value greater than 0.05, then we can conclude that the averages of two groups are significantly similar. Indeed the value of t-computed is less than the tabulated t-value for 8 degrees of freedom, which in R we can calculate:
This confirms that we can accept the null hypothesis H0 of equality of the means.

#Statistical power: the power of a hypothesis test is the probability that the test correctly rejects the null hypothesis. Statistical power has relevance only when the null is false.
#The higher the statistical power for a given experiment, the lower the probability of making a Type II (false negative) error.
#Low Statistical Power: Large risk of committing Type II errors, e.g. a false negative.
#High Statistical Power: Small risk of committing Type II errors.
#Resuming: Power analysis answers questions like “how much statistical power does my study have?” and “how big a sample size do I need?”.

M1<-mean(mm)                      # Mean for sample 1
M2<-mean(mf)                      # Mean for sample 2
S1<-sd(mm)                    # Std dev for sample 1
S2<-sd(mf)                      # Std dev for sample 2

Cohen.d <- (M1 - M2)/sqrt(((S1^2) + (S2^2))/2)  
                                         
library(pwr)
                                   
pwr.t.test(
       n = NULL,                   # Observations in _each_ group
       d = Cohen.d,            
       sig.level = 0.05,           # Type I probability
       power = 0.90,               # 1 minus Type II probability
       type = "two.sample",        # Change for one- or two-sample
       alternative = "two.sided")
       

#ANÁLISE DA INTERPRETAÇÃO DAS FRASES

# GENERO MASCULINO
genmasc<-data %>% filter(gender == 1, consent ==1)
qm6<-table(genmasc$VI)
iqm6<-cut(qm6,breaks=7)
head(iqm6)

qm7<-table(genmasc$VII)
iqm7<-cut(qm7,breaks=7)
head(iqm7)

qm8<-table(genmasc$VIII)
iqm8<-cut(qm8,breaks=7)
head(iqm8)

qm9<-table(genmasc$IX)
iqm9<-cut(qm9,breaks=7)
head(iqm9)

qm10<-table(genmasc$X)
iqm10<-cut(qm10,breaks=7)
head(iqm10)

qm11<-table(genmasc$XI)
iqm11<-cut(qm11,breaks=7)
head(iqm11)

MQR<-rowSums(cbind(qm6, qm7, qm8, qm9, qm10, qm11), na.rm=TRUE)
head(MQR)

# GENERO FEMININO
genfem<-data %>% filter(gender == 2, consent ==1)

qf6<-table(genfem$VI)
iqf6<-cut(qf6,breaks=7)

qf7<-table(genfem$VII)
iqf7<-cut(qf7,breaks=7)

qf8<-table(genfem$VIII)
iqf8<-cut(qf8,breaks=7)

qf9<-table(genfem$IX)
iqf9<-cut(qf9,breaks=7)

qf10<-table(genfem$X)
iqf10<-cut(qf10,breaks=7)

qf11<-table(genfem$XI)
iqf11<-cut(qf11,breaks=7)

FQR<-rowSums(cbind(qf6, qf7, qf8, qf9, qf10, qf11), na.rm=TRUE)

t.test(MQR,FQR,paired=TRUE)

var.test(MQR, FQR)


qtvmasc<-cut(MQR,breaks=7)
qtvfem<-cut(FQR,breaks=7)

summary(qtvmasc)
summary(qtvfem)

library(plotrix)
library(ggplot2)

xyqts<-c(18.4, 20.8, 20.1, 21.0, 19.7)
xxqts<-c(18.2, 23.3, 21.2, 20.5, 16.9)

qtslabels<-c("Strongly disagree", 	"Somewhat disagree",	"Neither Agree nor disagree",	"Somewhat agree",	"Strongly agree")

mcol<-color.gradient(c(0,0,0.5,1),c(0,0,0.5,1),c(1,1,0.5,1),9)
fcol<-color.gradient(c(1,1,0.5,1),c(0.5,0.5,0.5,1),c(0.5,0.5,0.5,1),9)
par(mar=pyramid.plot(xyqts,xxqts,labels=qtslabels,main="Gender Quotes",lxcol=mcol,rxcol=fcol,gap=10,show.values=TRUE))

pyramid.plot(xyqts,xxqts,labels=qtslabels,top.labels=c("MASCULIN","OPNION","FEMININ"),gap=4,lxcol="red",rxcol="blue", laxlab=c(0,5,10,15),raxlab=c(0,5,10,15))


write.csv(matrix(as.numeric(FQR),nrow=1),row.names=FALSE)
write.csv(matrix(as.numeric(MQR),nrow=1),row.names=FALSE)

var.test(MQR,FQR)
#qf(p, df.num, df.den)
qf(0.95, 4, 4)

#We obtained p-value greater than 0.05, then we can assume that the two variances are homogeneous. Indeed we can compare the value of F obtained with the tabulated value of F for alpha = 0.05, degrees of freedom of numerator = 4, and degrees of freedom of denominator = 4, using the function qf(p, df.num, df.den): Note that the value of F computed is less than the tabulated value of F, which leads us to accept the null hypothesis of homogeneity of variances.


t.test(MQR,FQR, var.equal=TRUE, paired=FALSE)
qt(0.975, 8)
qt(0.95, 8)

#We obtained p-value greater than 0.05, then we can conclude that the averages of two groups are significantly similar. Indeed the value of t-computed is less than the tabulated t-value for 8 degrees of freedom, which in R we can calculate:
This confirms that we can accept the null hypothesis H0 of equality of the means.


M1<-mean(MQR)                      # Mean for sample 1
M2<-mean(FQR)                      # Mean for sample 2
S1<-sd(MQR)                    # Std dev for sample 1
S2<-sd(FQR)                      # Std dev for sample 2

Cohen.d <- (M1 - M2)/sqrt(((S1^2) + (S2^2))/2)  
                                         
library(pwr)
                                   
pwr.t.test(
       n = NULL,                   # Observations in _each_ group
       d = Cohen.d,            
       sig.level = 0.05,           # Type I probability
       power = 0.90,               # 1 minus Type II probability
       type = "two.sample",        # Change for one- or two-sample
       alternative = "two.sided")


#ANÁLISE DE GENERO PELAS VARIAVEL IDADE

library(plotrix)
library(ggplot2)

genmasc<-data %>% filter(gender == 1, consent ==1)
agemasc<-genmasc$Age
edumasc<-genmasc$education
ctymasc<-genmasc$country

genfem<-data %>% filter(gender == 2, consent ==1)
agefem<-genfem$Age
edufem<-genfem$education
ctyfem<-genfem$country

write.csv(matrix(as.numeric(agemasc),nrow=1),row.names=FALSE)
write.csv(matrix(as.numeric(agefem),nrow=1),row.names=FALSE)

summary(agemasc)
summary(agefem)
cov(agemasc, agefem) #medida de variabilidade conjunta dessas duas variáveis aleatórias. Quando a covariâncias entre essas variáveis é positiva os dados apresentam tendência positiva na dispersão. Quando o valor da covariância é negativo, o comportamento é análogo, no entanto, os dados apresentam tendências negativas. 
cor(agemasc, agefem) # ASSOCIAÇÃO: Para ρ = 1, tem-se uma correlação perfeita entre as duas variáveis. Se ρ = - 1, há uma correlação perfeita entre as variáveis, no entanto, essa correlação é negativa. Caso ρ = 0, as duas variáveis não dependem linearmente uma da outra.

t.test(agemasc, agefem) #t-test, used to determine whether the means of two groups are equal to each other. The assumption for the test is that both groups are sampled from normal distributions with equal variances. The null hypothesis is that the two means are equal, and the alternative is that they are not. 

M1<-mean(agemasc)                      # Mean for sample 1
M2<-mean(agefem)                      # Mean for sample 2
S1<-sd(agemasc)                    # Std dev for sample 1
S2<-sd(agefem)                      # Std dev for sample 2

Cohen.d <- (M1 - M2)/sqrt(((S1^2) + (S2^2))/2)  
                                         
library(pwr)
                                   
pwr.t.test(
       n = NULL,                   # Observations in _each_ group
       d = Cohen.d,            
       sig.level = 0.05,           # Type I probability
       power = 0.90,               # 1 minus Type II probability
       type = "two.sample",        # Change for one- or two-sample
       alternative = "two.sided")


K = 1+3.3*(1+log10(7.7))
K
H = (53-15)/K
H

itvagemasc<-cut(agemasc,breaks=7)
itvagefem<-cut(agefem,breaks=7)
summary(itvagemasc)
summary(itvagefem)

xyage<-c(22.0, 18.1, 15.5, 6.5, 9.1, 10.4, 18.2)
xxage<-c(14.2, 12.9, 14.2, 14.3, 20.8, 9.1, 15.6)

agelabels<-c("(15-20)", "(20-25)", "(25-30)", "(30-35)", "(40-45)", "(45-50)", "(50-55)")

mcol<-color.gradient(c(0,0,0.5,1),c(0,0,0.5,1),c(1,1,0.5,1),7)
fcol<-color.gradient(c(1,1,0.5,1),c(0.5,0.5,0.5,1),c(0.5,0.5,0.5,1),7)
par(mar=pyramid.plot(xyage,xxage,labels=agelabels,main="Gender Age",lxcol=mcol,rxcol=fcol,gap=1,show.values=TRUE))


#ANÁLISE DE GENERO PELAS VARIAVEL IDADE E EDUCAÇÃO

edumasc<-genmasc$education
edufem<-genfem$education

write.csv(matrix(as.numeric(edumasc),nrow=1),row.names=FALSE)
write.csv(matrix(as.numeric(edufem),nrow=1),row.names=FALSE)

summary(edumasc)
summary(edufem)

var.test(edumasc,edufem)
#qf(p, df.num, df.den)
qf(0.95, 76, 77)

#We obtained p-value greater than 0.05, then we can assume that the two variances are homogeneous. Indeed we can compare the value of F obtained with the tabulated value of F for alpha = 0.05, degrees of freedom of numerator = 76, and degrees of freedom of denominator = 77, using the function qf(p, df.num, df.den): Note that the value of F computed is less than the tabulated value of F, which leads us to accept the null hypothesis of homogeneity of variances.


t.test(edumasc,edufem,var.equal=TRUE, paired=FALSE)
qt(0.975, 152.27)

#We obtained p-value greater than 0.05, then we can conclude that the averages of two groups are significantly similar. Indeed the value of t-computed is less than the tabulated t-value for 18 degrees of freedom, which in R we can calculate:
This confirms that we can accept the null hypothesis H0 of equality of the means.

M1<-mean(edumasc)                      # Mean for sample 1
M2<-mean(edufem)                      # Mean for sample 2
S1<-sd(edumasc)                    # Std dev for sample 1
S2<-sd(edufem)                      # Std dev for sample 2

Cohen.d <- (M1 - M2)/sqrt(((S1^2) + (S2^2))/2)  
                                         
library(pwr)
                                   
pwr.t.test(
       n = NULL,                   # Observations in _each_ group
       d = Cohen.d,            
       sig.level = 0.05,           # Type I probability
       power = 0.90,               # 1 minus Type II probability
       type = "two.sample",        # Change for one- or two-sample
       alternative = "two.sided")

K = 1+3.3*(1+log10(7.7))
K
H = (8-1)/K
H

itvedumasc<-cut(edumasc,breaks=7)
itvedufem<-cut(edufem,breaks=7)

summary(itvedumasc)
summary(itvedufem)

xyedu<-c(23.4,18.2,11.7,10.4,15.6,13.0,7.8)
xxedu<-c(23.1,10.3,15.4,6.4,15.4,14.1,15.4)

educationlabels<-c("(1-2)","(2-3)","(3-4)", "(4-5)","(5-6)","(6-7)","(7-8)")

mcol<-color.gradient(c(0,0,0.5,1),c(0,0,0.5,1),c(1,1,0.5,1),7)
fcol<-color.gradient(c(1,1,0.5,1),c(0.5,0.5,0.5,1),c(0.5,0.5,0.5,1),7)
par(mar=pyramid.plot(xyedu,xxedu,labels=educationlabels,main="Gender Education",lxcol=mcol,rxcol=fcol,gap=1,show.values=TRUE))































